{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first-pass trial notebook experimenting with creating a CNN for our FakeFaces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd # to read the csv files \n",
    "import os.path\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to allow for the memory limit to be able to grow (?) https://www.tensorflow.org/guide/gpu\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     8
    ]
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def _parse_image_from_filepath(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32) \n",
    "    return img, label\n",
    "\n",
    "# let's write a function that takes in the type of the dataset we want, and returns the tf_dataset \n",
    "def load_dataset_from_csv(setName):\n",
    "    # load setName.csv into a pandas dataframe\n",
    "    pd_dataset = pd.read_csv('data/archive/'+setName+'.csv')\n",
    "    pd_dataset.drop(['Unnamed: 0', 'original_path', 'label_str', 'path'], axis='columns', inplace=True)\n",
    "    \n",
    "    set_filenames = []\n",
    "    set_labels = []\n",
    "    # check if we haven't saved the filepath-label lists already; if not, then generate and pickle it for the future \n",
    "    if (not os.path.isfile(setName+'_paths_labels_lists.pkl')):\n",
    "        temp_filenames_list = []\n",
    "        temp_labels_list = []\n",
    "        for index, row in pd_dataset.iterrows():\n",
    "            img_details = row\n",
    "            img_label = img_details['label']\n",
    "            img_label_string = 'real' if (img_details['label'] == 1) else 'fake'\n",
    "            img_id = img_details['id']\n",
    "            img_filepath = 'data/archive/real_vs_fake/'+setName+'/'+img_label_string+'/'+str(img_id)+'.jpg'\n",
    "            if (not os.path.isfile(img_filepath)): # ignore instances in the pd that don't actually exist \n",
    "                continue \n",
    "            temp_filenames_list.append(img_filepath)\n",
    "            temp_labels_list.append(int(img_label))\n",
    "        filenames_labels_tuple = (temp_filenames_list, temp_labels_list)\n",
    "        with open(setName+'_paths_labels_lists.pkl', 'wb') as f:\n",
    "            pickle.dump(filenames_labels_tuple, f)\n",
    "        set_filenames = temp_filenames_list\n",
    "        set_labels = temp_labels_list\n",
    "    else: # otherwise, if we already have those lists, just grab them from disk\n",
    "        with open(setName+'_paths_labels_lists.pkl', 'rb') as f:\n",
    "            temp_tuple = pickle.load(f)\n",
    "            set_filenames = temp_tuple[0]\n",
    "            set_labels = temp_tuple[1]\n",
    "    \n",
    "    # prepare to turn the string paths and labels into a tf dataset\n",
    "    set_filenames = tf.constant(set_filenames)\n",
    "    set_labels = tf.constant(set_labels)\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((set_filenames, set_labels))\n",
    "    \n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    tf_dataset = tf_dataset.map(_parse_image_from_filepath)\n",
    "    tf_dataset = tf_dataset.cache()\n",
    "    tf_dataset = tf_dataset.shuffle(buffer_size=1000, seed=42, reshuffle_each_iteration=True) # might change this \n",
    "    tf_dataset = tf_dataset.batch(32, drop_remainder=False) # might change this too\n",
    "    tf_dataset = tf_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return tf_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = load_dataset_from_csv('train')\n",
    "valid_tf_dataset = load_dataset_from_csv('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves that in the labels, a 1 is real, while a 0 is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# let's try plotting some of the images \n",
    "# don't try running this; rigth now this doesnt work because I changed the setup a little, but you can change it a bit to get it to work if you are curious \n",
    "plt.figure(figsize=(10, 10))\n",
    "x = 1\n",
    "for i in dataset.sample(n=9, random_state=1).index:\n",
    "    ax = plt.subplot(3, 3, x)\n",
    "    x += 1\n",
    "    plt.imshow(mpimg.imread(train_img_filenames[i].numpy().decode('utf-8')))\n",
    "    img_label = 'real' if (int.from_bytes(train_img_labels[i].numpy(), byteorder='little')== 1) else 'fake'\n",
    "    plt.title('img ' + str(i) + ': ' + img_label)\n",
    "    plt.axis('off') \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just try adapting the cnn tutorial network for practice purposes \n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(256, 256, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(layers.Dense(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3008/3125 [===========================>..] - ETA: 24s - loss: 0.1660 - accuracy: 0.9919"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_tf_dataset, epochs=10, validation_data=valid_tf_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
