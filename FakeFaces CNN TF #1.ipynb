{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first-pass trial notebook experimenting with creating a CNN for our FakeFaces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd # to read the csv files \n",
    "import os.path\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to allow for the memory limit to be able to grow (?) https://www.tensorflow.org/guide/gpu\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99999 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# maybe we should use IMG instead...\n",
    "# thanks so much to Alex Kyllo for pointing this out to us!\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "rescale_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = rescale_datagen.flow_from_directory('data/archive/real_vs_fake/train',\n",
    "                                                      class_mode='binary',\n",
    "                                                     batch_size=32, \n",
    "                                                     shuffle=True,\n",
    "                                                     seed=42)\n",
    "validation_generator = rescale_datagen.flow_from_directory('data/archive/real_vs_fake/valid',\n",
    "                                                      class_mode='binary',\n",
    "                                                     batch_size=32, \n",
    "                                                     shuffle=True,\n",
    "                                                     seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves that in the labels, a 1 is real, while a 0 is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# let's try plotting some of the images \n",
    "# don't try running this; rigth now this doesnt work because I changed the setup a little, but you can change it a bit to get it to work if you are curious \n",
    "# plt.figure(figsize=(10, 10))\n",
    "# x = 1\n",
    "# for i in dataset.sample(n=9, random_state=1).index:\n",
    "#     ax = plt.subplot(3, 3, x)\n",
    "#     x += 1\n",
    "#     plt.imshow(mpimg.imread(train_img_filenames[i].numpy().decode('utf-8')))\n",
    "#     img_label = 'real' if (int.from_bytes(train_img_labels[i].numpy(), byteorder='little')== 1) else 'fake'\n",
    "#     plt.title('img ' + str(i) + ': ' + img_label)\n",
    "#     plt.axis('off') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just try adapting the cnn tutorial network for practice purposes \n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(256, 256, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(64, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(32, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = models.Sequential()\n",
    "model_vgg.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(256, 256, 3)))\n",
    "model_vgg.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.MaxPooling2D((2, 2)))\n",
    "model_vgg.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.MaxPooling2D((2, 2)))\n",
    "model_vgg.add(layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.MaxPooling2D((2, 2)))\n",
    "model_vgg.add(layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model_vgg.add(layers.MaxPooling2D((2, 2)))\n",
    "model_vgg.add(layers.Flatten())\n",
    "model_vgg.add(layers.BatchNormalization())\n",
    "model_vgg.add(layers.Dense(4096, activation='elu', kernel_initializer='he_normal'))\n",
    "model_vgg.add(layers.BatchNormalization())\n",
    "model_vgg.add(layers.Dense(4096, activation='elu', kernel_initializer='he_normal'))\n",
    "model_vgg.add(layers.BatchNormalization())\n",
    "model_vgg.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7519WARNING:tensorflow:From C:\\Users\\valen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 1003s 321ms/step - loss: 0.5288 - accuracy: 0.7519 - val_loss: 0.5384 - val_accuracy: 0.7750\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9092INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 998s 319ms/step - loss: 0.2249 - accuracy: 0.9092 - val_loss: 0.3069 - val_accuracy: 0.8826\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9526INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 993s 318ms/step - loss: 0.1241 - accuracy: 0.9526 - val_loss: 0.1576 - val_accuracy: 0.9378\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9695INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 1072s 343ms/step - loss: 0.0822 - accuracy: 0.9695 - val_loss: 0.0813 - val_accuracy: 0.9711\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9787INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 1189s 381ms/step - loss: 0.0584 - accuracy: 0.9787 - val_loss: 0.0589 - val_accuracy: 0.9797\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 1044s 334ms/step - loss: 0.0468 - accuracy: 0.9830 - val_loss: 0.1051 - val_accuracy: 0.9659\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9866INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 1040s 333ms/step - loss: 0.0374 - accuracy: 0.9866 - val_loss: 0.0550 - val_accuracy: 0.9816\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 1045s 334ms/step - loss: 0.0334 - accuracy: 0.9883 - val_loss: 0.0932 - val_accuracy: 0.9726\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9897INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 1053s 337ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.0460 - val_accuracy: 0.9856\n",
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 1015s 325ms/step - loss: 0.0248 - accuracy: 0.9912 - val_loss: 0.0463 - val_accuracy: 0.9872\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 1000s 320ms/step - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.1057 - val_accuracy: 0.9712\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 1000s 320ms/step - loss: 0.0187 - accuracy: 0.9935 - val_loss: 0.0590 - val_accuracy: 0.9822\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 1087s 348ms/step - loss: 0.0202 - accuracy: 0.9929 - val_loss: 0.0848 - val_accuracy: 0.9751\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 1092s 349ms/step - loss: 0.0146 - accuracy: 0.9950 - val_loss: 0.0498 - val_accuracy: 0.9858\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 991s 317ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.0509 - val_accuracy: 0.9859\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9950INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 1004s 321ms/step - loss: 0.0138 - accuracy: 0.9950 - val_loss: 0.0416 - val_accuracy: 0.9876\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 1033s 331ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.0461 - val_accuracy: 0.9872\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9959INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 1090s 349ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.0414 - val_accuracy: 0.9886\n",
      "Epoch 19/50\n",
      "3125/3125 [==============================] - 1369s 438ms/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 0.0559 - val_accuracy: 0.9900\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 948s 303ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0383 - val_accuracy: 0.9894\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 932s 298ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0613 - val_accuracy: 0.9847\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9973INFO:tensorflow:Assets written to: models/model_vgg\\assets\n",
      "3125/3125 [==============================] - 963s 308ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0351 - val_accuracy: 0.9888\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 1096s 351ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0547 - val_accuracy: 0.9893\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 999s 320ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0381 - val_accuracy: 0.9859\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 1019s 326ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0573 - val_accuracy: 0.9856\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 1047s 335ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0584 - val_accuracy: 0.9903\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 1016s 325ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.1825 - val_accuracy: 0.9832\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 1039s 332ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.5617 - val_accuracy: 0.9747\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 1525s 488ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.5304 - val_accuracy: 0.9645\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 1063s 340ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.1781 - val_accuracy: 0.9869\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 1058s 339ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 1.3625 - val_accuracy: 0.9852\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 1372s 439ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 1.3809 - val_accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "model_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "model_vgg_history = model_vgg.fit(train_generator, \n",
    "                                  epochs=50, \n",
    "                                  validation_data=validation_generator, \n",
    "                                  callbacks=[\n",
    "                                      tf.keras.callbacks.ModelCheckpoint('models/model_vgg', save_best_only=True),\n",
    "                                      tf.keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True),\n",
    "                                      tf.keras.callbacks.TensorBoard(get_run_logdir())\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = rescale_datagen.flow_from_directory('data/archive/real_vs_fake/test',\n",
    "                                                      class_mode='binary',\n",
    "                                                     batch_size=32, \n",
    "                                                     shuffle=True,\n",
    "                                                     seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 139s 223ms/step - loss: 0.0316 - accuracy: 0.9897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03162318840622902, 0.9897000193595886]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg.evaluate(test_generator, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
